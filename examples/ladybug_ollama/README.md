# Ladybug + Ollama Example

This example demonstrates using predicato with a fully local setup combining:

- **Ladybug**: Embedded graph database (no server required)
- **Ollama**: Local LLM inference (no cloud API required)
- **OpenAI Embeddings**: Could be replaced with local embeddings for complete locality

## Benefits of This Setup

### üîí **Maximum Privacy**
- All graph data stays local in embedded Ladybug database
- All LLM processing happens locally with Ollama
- Only embeddings use external API (replaceable with local service)

### ‚ö° **High Performance**
- Embedded database eliminates network latency
- Local LLM avoids API rate limits
- No internet dependency for core operations

### üí∞ **Cost Effective**
- No cloud database hosting costs
- No per-token LLM API charges (except embeddings)
- Run on your own hardware

### üõ†Ô∏è **Development Friendly**
- No complex server setup
- Easy to version control database with your code
- Perfect for development and testing

## Prerequisites

### Required
- Go 1.24+
- [Ollama](https://ollama.ai/) installed and running
- OpenAI API key (for embeddings)

### Optional (for fully local setup)
- Local embedding service to replace OpenAI

## Setup Instructions

### 1. Install Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download from https://ollama.ai/

### 2. Start Ollama and Pull Model

```bash
# Start Ollama server
ollama serve

# In another terminal, pull a model
ollama pull llama2:7b

# Verify it works
ollama run llama2:7b "Hello world"
```

### 3. Set Environment Variables

```bash
# Required for embeddings (until replaced with local service)
export OPENAI_API_KEY="your-openai-api-key"
```

### 4. Run the Example

```bash
cd examples/ladybug_ollama
go run main.go
```

## Expected Output

```
üöÄ Starting predicato example with Ladybug + Ollama
   This example demonstrates a fully local setup:
   - Ladybug: embedded graph database
   - Ollama: local LLM inference
   - OpenAI: embeddings (could be replaced with local)

üìä Setting up Ladybug embedded graph database...
   ‚úÖ Ladybug driver created (embedded database at ./example_graph.db)

üß† Setting up Ollama local LLM client...
   ‚úÖ Ollama client created (using llama2:7b model)
   üí° Make sure Ollama is running: `ollama serve`
   üí° Make sure model is available: `ollama pull llama2:7b`

üî§ Setting up embedding client...
   ‚úÖ OpenAI embedder created (text-embedding-3-small)
   üí° For fully local setup, replace with local embedding service

üåê Setting up Predicato client with local components...
   ‚úÖ Predicato client created with local Ladybug + Ollama setup

üìù Adding example episodes to the knowledge graph...
   ‚ö†Ô∏è  Expected error with stub implementation: LadybugDriver not implemented
   This will work once the Ladybug Go library is available

üîç Searching the knowledge graph...
   ‚ö†Ô∏è  Expected errors with stub implementation
   This will work once the Ladybug Go library is available

üí≠ Testing Ollama LLM integration...
   Sending query to Ollama...
   ‚úÖ Ollama response received:
     Embedded graph databases like Ladybug offer several advantages over server-based solutions...
     Used 245 tokens

üìã Example Summary:
   ‚úÖ Ladybug driver: Created (stub implementation)
   ‚úÖ Ollama client: Created and tested
   ‚úÖ Predicato integration: Demonstrated

üéâ Example completed successfully!
```

## Current Status

### What Works Now ‚úÖ
- Ladybug driver creation (stub implementation)
- Ollama LLM client integration
- OpenAI embeddings
- Complete API demonstration

### What Will Work Later üîÆ
- Actual graph database operations (when Ladybug Go library is available)
- Full knowledge graph storage and retrieval
- Hybrid search with local graph traversal

## Configuration Options

### Different Ollama Models

```go
// Larger model for better quality
llmConfig := llm.Config{
    Model: "llama2:13b",  // or "codellama:7b", "mistral:7b", etc.
    Temperature: &[]float32{0.5}[0],  // Lower for more focused responses
    MaxTokens: &[]int{2000}[0],       // Longer responses
}
```

### Custom Ollama URL

```go
// If Ollama is running on different host/port
ollama, err := llm.NewOllamaClient("http://192.168.1.100:11434", "llama2:7b", llmConfig)
```

### Different Ladybug Database Path

```go
// Custom database location
ladybugDriver, err := driver.NewLadybugDriver("/path/to/my/graph.db")
```

## Troubleshooting

### Ollama Issues

**Problem**: `connection refused`
```bash
# Make sure Ollama is running
ollama serve

# Check if it's responding
curl http://localhost:11434/api/tags
```

**Problem**: `model not found`
```bash
# List available models
ollama list

# Pull the required model
ollama pull llama2:7b
```

**Problem**: Slow responses
- Try smaller model: `llama2:7b` instead of `llama2:13b`
- Reduce `MaxTokens` in config
- Ensure sufficient RAM (8GB+ recommended)

### Ladybug Issues

**Current**: All Ladybug operations return "not implemented" errors - this is expected until the Ladybug Go library is available.

**Future**: Once available, potential issues might include:
- Database file permissions
- Disk space for database files
- CGO compilation requirements

### Memory Usage

This setup can be memory-intensive:
- **Ollama models**: 4-8GB RAM (depending on model size)
- **Embeddings**: Temporary memory for batch processing
- **Ladybug database**: Memory-mapped files

**Recommendations**:
- Start with `llama2:7b` model (smaller)
- Monitor system resources
- Consider using swap if RAM is limited

## Performance Comparison

| Component | Local (This Setup) | Cloud Alternative | Notes |
|-----------|-------------------|-------------------|--------|
| Graph DB | Ladybug (embedded) | Neo4j (server) | Local: faster queries, no network |
| LLM | Ollama (local) | OpenAI API | Local: no rate limits, slower inference |
| Embeddings | OpenAI API | OpenAI API | Could be local in future |
| **Overall** | **Privacy + Control** | **Speed + Convenience** | Trade-offs depend on use case |

## Future Enhancements

### Complete Local Setup
```go
// Replace OpenAI embeddings with local service
localEmbedder := embedder.NewLocalEmbedder("http://localhost:8080", embedderConfig)
```

### Advanced Ollama Configuration
```go
// Custom system prompts for graph-specific tasks
systemPrompt := `You are an AI assistant specialized in analyzing temporal knowledge graphs. 
Focus on relationships between entities and temporal patterns in the data.`

llmConfig := llm.Config{
    Model: "llama2:7b",
    Temperature: &[]float32{0.3}[0],  // More focused for graph analysis
    Stop: []string{"</analysis>", "\n\n"},  // Custom stop sequences
}
```

### Production Considerations
- Database backup strategies for Ladybug files
- Model version management for Ollama
- Resource monitoring and scaling
- Error recovery and fallback mechanisms

## Related Examples

- **[Basic Example](../basic/)**: Neo4j + OpenAI setup
- **[OpenAI Compatible](../openai_compatible/)**: Various local LLM services

## Resources

- **Ladybug Documentation**: https://docs.LadybugDB.com/
- **Ollama Models**: https://ollama.ai/library
- **Local Embeddings**: Consider sentence-transformers, BGE, or similar